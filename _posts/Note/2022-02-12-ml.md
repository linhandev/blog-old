---
layout: "post"
author: Lin Han
title: "ml"
date: "2022-02-12 02:54"
math: true
categories:
  -
tags:
  -
public: false
---

# 线性回归
- 很多自然现象是线性关系
- 一个非线性关系在很小的范围内也可以用线性关系描述：切线
- 对高斯随机的关系线性模型可以表示所有的参数
[//]: # (TODO: 高斯随机)
- 计算简单，容易理解

# 单参数
符号
- $\bar{x}$：平均值
- $s_{x}^2=s_{xx}$：方差
- $s_{x}=\sqrt{s_{xx}}$：标准差
- $s_{xy}$：协方差

$$
\begin{align}
y&=\beta_{0}+\beta_{1}x+\epsilon \\
\hat{y} &= \beta_0 + \beta_1 x \\
\epsilon_{i} &= y_{i}-\hat{y}_{i}
\end{align}
$$

RSS: $\epsilon$的平方和，不除N。Residual Sum of Square，Squared Residuals(SSR)，Sum of Squared Errors(SSE)

$$
RSS(\beta_{0}, \beta_{1})=\sum_{i=1}^{N}(y-\hat{y})^2
$$


Least Square Fit：最小化RSS，点和直线的垂直距离平方和最小

均值
$$
\bar{x} = \frac{1}{N}\sum_{i=1}^{N}x_{i} \\
\bar{y} = \frac{1}{N}\sum_{i=1}^{N}y_{i} \\
$$

xy协方差
$$
s_{xy}=\frac{1}{N}\sum_{i=1}^{N}[(x_{i}-\bar{x})*(y_{i}-\bar{y})]
$$

xy协方差是 $(x-\bar{x})$ 和 $(y-\bar{y})$ 两个向量的内积

x方差
$$
\begin{align}
s_{xx} &= s_{x}^2=\frac{1}{N}\sum_{i=1}^{N}(x_{i}-\bar{x})^{2} \\
s_{x} &= \sqrt{s_{xx}}
\end{align}
$$
y方差
$$
\begin{align}
s_{yy} &= s_{y}^2=\frac{1}{N}\sum_{i=1}^{N}(y_{i}-\bar{y})^{2} \\
s_{y} &= \sqrt{s_{yy}}
\end{align}
$$

方差 = 平方和平均值 - 平均值的平方
$$
\begin{align}
s_{xx}&=\frac{1}{N}\sum_{i=1}^{N}(x_{i}-\bar{x})^2\\
&=\frac{1}{N}\sum_{i=1}^{N}(x_{i}^2-2x_{i}\bar{x}+\bar{x}^2)\\
&=\frac{1}{N}\sum_{i=1}^{N} x_{i}^2 -2\bar{x}\frac{1}{N}\sum_{i=1}^{N}{x_{i}} + \bar{x}^2\\
&=\frac{1}{N}\sum_{i=1}^{N} x_{i}^2 - \bar{x}^2
\end{align}
$$

x，y标准差相当于 $(x-\bar{x}),(y-\bar{y})$ 的长度

根据柯西不等式，$s_{xy} = s_{x}*s_{y}*cos(x,y) \le s_{x}*s_{y}$

关联系数 $r_{xy}=\frac{s_{xy}}{s_{x}s_{y}} \in [-1, 1]$

$$
\beta_1 = \frac{s_{xy}}{s_{xx}}=\frac{r_{xy}s_{y}}{s_{x}}, \quad \beta_0 = \bar{y} - \beta_1\bar{x}
$$

$$
RSS(\beta_{0}, \beta{1})=\sum_{i=1}^{N} (y_{i}-\beta_{0}-\beta_{1}x_{i})^2
$$
二次函数是凸函数，所以求RSS最小值等价于求RSS导数为0的点

$$
\begin{align}
\frac{\partial RSS(\beta_{0}, \beta_{1})}{\partial \beta_{0}}&=\sum_{i=1}^{N}2*(y_{i}-\beta_{0}-\beta_{1}x_{i})*-1 \\
&=-2*\sum_{i=1}^{N}\epsilon_{i}\\
&=0 \\
\frac{\partial RSS(\beta_{0}, \beta_{1})}{\partial \beta_{1}} &= \sum_{i=1}^{N}2*(y_{i}-\beta_{0}-\beta_{1}x_{i})*(-x_{i}) \\
&=-2*\sum_{i=1}^{N}\epsilon_{i}*x_{i}&\\
&=0 \\
\end{align}
$$




$$
min RSS(\beta_{0}, \beta_{1})=N(1-r_{xy}^2)s_{y}^2
$$
essense of linear algebra
https://www.youtube.com/watch?v=kjBOesZCoqc&list=PL0-GT3co4r2y2YErbmuJw2L5tW4Ew2O5B


# 多参数
可能有多个输入决定一个输出

符号

- $x=(x_{1}, x_{2}, ... , x_{k})$
- $\hat{y}=\beta_{0} + \beta_{1}x_{1} + ... + \beta_{k}x_{k}$

$$
\begin{align}
X&=
\begin{bmatrix}
1 & x_{1,1} & ... & x_{1,k} \\
. & . & . & .\\
. & . & . & .\\
. & . & . & .\\
1 & x_{N,1} & ... & x_{N,k}
\end{bmatrix} \\
Y&=
\begin{bmatrix}
y_{1} \\
. \\
. \\
. \\
y_{N}
\end{bmatrix} \\
\beta&=
\begin{bmatrix}
\beta_{0} \\
\beta_{1} \\
. \\
. \\
. \\
\beta_{k}s
\end{bmatrix} \\
\end{align}
$$

$$
\begin{align}
Y \approx \hat{Y} &= X\beta \\
\begin{bmatrix}
y_{1} \\
. \\
. \\
. \\
y_{N}
\end{bmatrix}
\approx
\begin{bmatrix}
\hat{y}_{1} \\
. \\
. \\
. \\
\hat{y}_{N}
\end{bmatrix}
&=
\begin{bmatrix}
1 & x_{1,1} & ... & x_{1,k} \\
. & . & . & .\\
. & . & . & .\\
. & . & . & .\\
1 & x_{N,1} & ... & x_{N,k}
\end{bmatrix} \begin{bmatrix}
\beta_{0} \\
\beta_{1} \\
. \\
. \\
. \\
\beta_{k}s
\end{bmatrix}
\end{align}
$$

RSS: Residual Square of Sum，不除N
MSE: Mean Square Error，除N
Normalized MSE：$\frac{MSE}{s_{y}^{2}}\in[0,1]$
$$
\begin{align}
\epsilon &= y-\hat{y} \\
RSS(\beta)&=\epsilon^{T}\cdot\epsilon \\
MSE(\beta)&=\frac{1}{N}\epsilon^{T}\cdot\epsilon \\
NormalizedMSE&=\frac{1}{N}\frac{\epsilon^{T}\cdot\epsilon}{s_{y}^{2}}\in[0,1]
\end{align}
$$

$$
\beta=(X^{T}X)^{-1}X^{T}Y
$$
