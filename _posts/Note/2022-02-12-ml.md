---
layout: "post"
author: Lin Han
title: "ml"
date: "2022-02-12 02:54"
math: true
categories:
  -
tags:
  -
public: false
---

# 线性回归
- 很多自然现象是线性关系
- 一个非线性关系在很小的范围内也可以用线性关系描述：切线
- 计算简单，容易理解

符号
- $\bar{x}$：平均值
- $s_{x}^2=s_{xx}$：方差
- $s_{x}=\sqrt{s_{xx}}$：标准差
- $s_{xy}$：协方差

$$
\hat{y} = \beta_0 + \beta_1 x
$$

$$
y=\beta_{0}+\beta_{1}x+\epsilon
$$

$\epsilon$是推理值和实际值之间的差

RSS: $\epsilon$的平方和，Residual Sum of Square，Squared Residuals(SSR)，Sum of Squared Errors(SSE)

Least Square Fit：最小化RSS，y和直线的垂直距离平方和最小

$$
RSS(\beta{0}, \beta{1})=\sum_{i=1}^{N}(y-\hat{y})^2
$$

xy协方差
$$
s_{xy}=\frac{1}{N}\sum_{i=1}^{N}[(x_{i}-\bar{x})*(y_{i}-\bar{y})]
$$

xy协方差相当于 $(x-\bar{x})$ 和 $(y-\bar{y})$ 两个向量的内积

x方差
$$
s_{xx}=s_{x}^2=\frac{1}{N}\sum_{i=1}^{N}(x_{i}-\bar{x})^{2}
$$

$$
s_{xx}=\frac{1}{N}\sum(x_{i}-\bar{x})^2=\frac{1}{N}\sum(x_{i}^2-2x_{i}\bar{x}+\bar{x}^2)=\frac{1}{N}\sum x_{i}^2 -2\bar{x}\frac{1}{N}\sum{x_{i}} + \bar{x}^2=\frac{1}{N}\sumx_{i}^2 - \bar{x}^2
$$

方差 = 平方和平均值 - 平均值的平方

x标准差相当于 $(x-\bar{x}),(y-\bar{y})$ 的长度

y方差
$$
s_{yy}=s_{y}^2=\frac{1}{N}\sum_{i=1}^{N}(y_{i}-\bar{y})^{2}
$$

根据柯西不等式，$s_{xy} \le s_{x}*s_{y}$

相关性 $r_{xy}=\frac{s_{xy}}{s_{x}s_{y}} \in [-1, 1]$

$$
\beta_1 = s_{xy}/s_{xx}=\frac{r_{xy}s_{y}}{s_{x}}, \quad \beta_0 = \bar{y} - \beta_1\bar{x}
$$

$$
RSS(\beta_{0}, \beta{1})=\frac{1}{N}\sum (y_{i}-\beta_{0}-\beta_{1}x_{i})^2
$$
二次函数是凸函数，所以求RSS最小值等价于求RSS导数为0的点

$$
\frac{\partial RSS}{\beta_{0}}=\frac{1}{N}\sum(y_{i}-\beta_{0}-\beta_{1}x_{i})*-1=-\frac{1}{N}\sum\epsilon_{i}=0
$$

$$
\frac{\partial RSS}{\beta_{1}} = \frac{1}{N}\sum(y_{i}-\beta_{0}-\beta_{1}x_{i})*(-x_{i})=-\frac{1}{N}\sum\epsilon_{i}*x_{i}=0
$$



$$
min RSS(\beta_{0}, \beta_{1})=N(1-r_{xy}^2)s_{y}^2
$$
essense of linear algebra
https://www.youtube.com/watch?v=kjBOesZCoqc&list=PL0-GT3co4r2y2YErbmuJw2L5tW4Ew2O5B
